{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/marbles/Projects/miniconda/envs/unsloth_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Your Flash Attention 2 installation seems to be broken?\n",
      "A possible explanation is you have a new CUDA version which isn't\n",
      "yet compatible with FA2? Please file a ticket to Unsloth or FA2.\n",
      "We shall now use Xformers instead, which does not have any performance hits!\n",
      "We found this negligible impact by benchmarking on 1x A100.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 8192 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "cache_dir = \"/home/ubuntu/marbles/Projects/narrative-dataset/model_cache-2\"\n",
    "model_name = \"unsloth/Llama-3.3-70B-Instruct\"\n",
    "train_dataset_path = \"./arc-tp_data_train.json\"\n",
    "val_dataset_path = \"./arc-tp_data_val.json\"\n",
    "test_dataset_path = \"./arc-tp_data_test.json\"\n",
    "lora_save_name = \"70b-4bit-lora-r128\"\n",
    "lora_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.48.0.\n",
      "   \\\\   /|    GPU: NVIDIA H100 PCIe. Max memory: 79.097 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 9.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:29<00:00, 11.18s/it]\n",
      "Unsloth 2025.1.5 patched 80 layers with 80 QKV layers, 80 O layers and 80 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_dim, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples: List[Dict[str, Any]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Format prompts by appending EOS token to complete-text.\n",
    "    \"\"\"\n",
    "    output_texts = []\n",
    "    for example in examples:\n",
    "        text = example['complete-text'] + tokenizer.eos_token\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "def load_json_data(json_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load narrative data from a JSON file.\n",
    "    This function wraps multiple JSON objects into a list if they aren't already.\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read().strip()\n",
    "        try:\n",
    "            data = json.loads(content)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "            data = []\n",
    "    \n",
    "    return data\n",
    "\n",
    "train_data = load_json_data(train_dataset_path)\n",
    "formatted_train_texts = formatting_prompts_func(train_data)\n",
    "train_dataset = Dataset.from_list([{\"text\": text} for text in formatted_train_texts])\n",
    "\n",
    "# Load and format validation data  \n",
    "val_data = load_json_data(val_dataset_path)\n",
    "formatted_val_texts = formatting_prompts_func(val_data)\n",
    "val_dataset = Dataset.from_list([{\"text\": text} for text in formatted_val_texts])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14351"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the longest text in the dataset\n",
    "max_len = 0\n",
    "longest_text = \"\"\n",
    "for example in val_dataset:\n",
    "    if len(example[\"text\"]) > max_len:\n",
    "        max_len = len(example[\"text\"])\n",
    "        longest_text = example[\"text\"]\n",
    "len(longest_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/marbles/Projects/miniconda/envs/unsloth_env/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/ubuntu/marbles/Projects/miniconda/envs/unsloth_env/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 350/350 [00:02<00:00, 125.24 examples/s]\n",
      "Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:01<00:00, 47.01 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 30,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        evaluation_strategy = \"steps\",\n",
    "        eval_steps = 5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 350 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 30\n",
      " \"-____-\"     Number of trainable parameters = 1,656,750,080\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 29:21, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.537900</td>\n",
       "      <td>1.442681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.174800</td>\n",
       "      <td>1.149533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.064200</td>\n",
       "      <td>1.042795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.973600</td>\n",
       "      <td>1.017910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.931500</td>\n",
       "      <td>1.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.992900</td>\n",
       "      <td>1.004524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('70b-4bit-lora-r128/tokenizer_config.json',\n",
       " '70b-4bit-lora-r128/special_tokens_map.json',\n",
       " '70b-4bit-lora-r128/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import unsloth_train\n",
    "trainer_stats = unsloth_train(trainer)\n",
    "model.save_pretrained(lora_save_name)  # Local saving\n",
    "tokenizer.save_pretrained(lora_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mval_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m### Response:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "val_dataset[0][\"text\"].split(\"### Response:\\n\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/marbles/Projects/miniconda/envs/unsloth_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"70b-4bit-lora-r128\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>### INSTRUCTIONS\n",
      "Analyze the story and classify it into one of the story arc types based on the protagonist's condition at each turning point. Explain your reasoning step by step.\n",
      "\n",
      "### INPUT \n",
      "Story, broken down into numbered sentences:\n",
      "1) John, a successful attorney of high repute, indulged in the splendour of his wealthy lifestyle.\n",
      "2) Despite these materialistic achievements, he often found himself ensnared in a persistent inner conflict.\n",
      "3) The unfamiliar unfamiliarity of this longing proved to be taxing.\n",
      "4) However, his seemingly perfect life was disrupted when a severe financial crisis erupted.\n",
      "5) His life partner, a woman enamoured by materialistic luxuries and societal impressions, found herself unable to cope with the weight of their sudden financial downfall.\n",
      "6) Stripped of the pride of wealth, the respect of his clients, and the company of his wife, John was left alone, staring at the stark reality of a humble existence devoid of any superficial charm.\n",
      "7) Amidst this solitude, the voice he had once dismissed echoed louder, underscoring the harsh reality of his vulnerabilities and the severe blow to his self-worth.\n",
      "8) Lost and aimless, John serendipitously encountered Paul during one of his routine wanderings.\n",
      "9) Paul's simple living and modest exposure starkly contrasted the luxurious life John had once savoured.\n",
      "10) Sensing John's turmoil, Paul kindly extended his hand to lead him towards a path of spiritual awakening, promising him solace and tranquillity.\n",
      "11) Longing for an antidote to his pain, John graciously welcomed Paul's assistance and embarked on a journey wherein he poured over various spiritual scriptures and dived deep into religious teachings.\n",
      "12) His quest for spiritual understanding acquired a social dimension when he began attending religious gatherings at the local church.\n",
      "13) At one such congregation, he met Susan, a woman whose compassionate nature bore the evidence of suffering that echoed John's own.\n",
      "14) Their relationship evolved beyond a mere acquaintanceship.\n",
      "15) Their shared experiences led to a bond that offered them mutual peace and understanding.\n",
      "16) However, their flourishing association was hit with an unexpected tragedy; Susan confided in John, revealing her battle against a life-threatening illness.\n",
      "17) Torn by the impending doom, he sought refuge in relentless prayers for Susan's well-being.\n",
      "18) Despite her impending mortality, Susan's faith remained unbroken.\n",
      "19) As her body deteriorated, her spirit remained strong, offering John an inspiring illustration of bravery.\n",
      "20) Susan's remaining days were marked by physical suffering, but her final moments were met with courageous serenity, her faith standing unwavering against the cruel hands of fate.\n",
      "21) Susan's demise left John grappling with unsettling void, yet her unwavering spirit and brave acceptance had indelibly altered his perception of life and faith.\n",
      "22) After battling with his grief, John wholeheartedly surrendered himself to his newfound spirituality.\n",
      "23) The once-ignored voice within him now functioned as a guiding light on his spiritual journey.\n",
      "24) His perspective altered by his trials, he resolved to spend his remaining life serving others, helping them navigate through their struggles.\n",
      "25) The film culminates with John delivering an invigorating sermon to the reverent church congregation as a newly-ordained pastor.\n",
      "26) \"Our true journey begins within,\" he asserted, embodying his deep understanding that true peace and satisfaction stem from unfaltering faith.\n",
      "\n",
      "Story Arc Types:\n",
      "- Rags to Riches: Protagonist starts disadvantaged and ends better (e.g., 0â†’1â†’2â†’4â†’10)\n",
      "- Riches to Rags: Protagonist starts high and ends lower (e.g., 10â†’9â†’8â†’6â†’0)\n",
      "- Man in a Hole: Protagonist falls into trouble but recovers (e.g., 6â†’2â†’1â†’4â†’10)\n",
      "- Icarus: Protagonist rises then falls dramatically (e.g., 2â†’4â†’9â†’5â†’1)\n",
      "- Double Man in a Hole: Two cycles of fall and recovery (e.g., 6â†’2â†’7â†’4â†’10)\n",
      "- Cinderella: Rise, setback, ultimate triumph (e.g., 1â†’7â†’4â†’1â†’10)\n",
      "- Oedipus: Start high, fall, recover, final fall (e.g., 10â†’4â†’7â†’9â†’1)\n",
      "\n",
      "Ground Truth Turning Points:\n",
      "- Opportunity (tp1): 5.0 - Introductory event that occurs after presenting the setting and background of the main characters.\n",
      "- Change of Plans (tp2): 10.0 - Event where the main goal of the story is defined, starting the main action.\n",
      "- Point of No Return (tp3): 12.0 - Event that pushes the main character(s) to fully commit to their goal.\n",
      "- Major Setback (tp4): 16.0 - Event where things fall apart temporarily or permanently.\n",
      "- Climax (tp5): 24.0 - Final event/resolution of the main story (the \"biggest spoiler\").\n",
      "\n",
      "    At the end of your reasoning, simply state the determined story arc.\n",
      "    ### RESPONSE\n",
      "    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ### 1. Identify the protagonist in the story\n",
      "The protagonist in the story is John, a successful attorney who undergoes significant personal and spiritual changes throughout the narrative.\n",
      "\n",
      "### 2. At the sentence indicated by the first turning point, state the sentence again, and describe the protagonist's state.\n",
      "The first turning point (tp1) is at sentence 5: \"His life partner, a woman enamoured by materialistic luxuries and societal impressions, found herself unable to cope with the weight of their sudden financial downfall.\"\n",
      "At this point, John's life is being disrupted by a severe financial crisis, and his partner is struggling to cope with the loss of their wealthy lifestyle. This indicates a significant downturn in John's personal and financial life, suggesting his condition is worsening. Approximate condition: 4\n",
      "\n",
      "### 3. Identify the second turning point and state the sentence corresponding to that turning point, and describe how it changed relative to the first turning point.\n",
      "The second turning point (tp2) is at sentence 10: \"Sensing John's turmoil, Paul kindly extended his hand to lead him towards a path of spiritual awakening, promising him solace and tranquillity.\"\n",
      "Relative to the first turning point, John's condition has potentially hit a low due to his financial crisis and personal losses. However, with Paul's offer, John is presented with an opportunity for positive change and spiritual growth. This marks a point where John begins to seek a way out of his turmoil, indicating a slight improvement in his outlook or condition. Approximate condition: 5\n",
      "\n",
      "### 4. At the third turning point, state the sentence corresponding to that turning point, and describe how it changed relative to the second turning point.\n",
      "The third turning point (tp3) is at sentence 12: \"At one such congregation, he met Susan, a woman whose compassionate nature bore the evidence of suffering that echoed John's own.\"\n",
      "Compared to the second turning point, John has started on his spiritual journey and is now engaging with a community and meeting someone (Susan) who shares similar experiences of suffering. This connection and the community involvement suggest a further improvement in John's condition as he finds support and understanding. Approximate condition: 7\n",
      "\n",
      "### 5. Identify the fourth turning point and state the sentence corresponding to that turning point, and describe how it changed relative to the third turning point.\n",
      "The fourth turning point (tp4) is at sentence 16: \"However, their flourishing association was hit with an unexpected tragedy; Susan confided in John, revealing her battle against a life-threatening illness.\"\n",
      "Relative to the third turning point, John's condition worsens as he faces a new and significant challenge with Susan's illness. This setback introduces a period of grief and concern for John, indicating a decline in his condition. Approximate condition: 3\n",
      "\n",
      "### 6. At the fifth turning point, state the sentence corresponding to that turning point, and describe how it changed relative to the fourth turning point.\n",
      "The fifth turning point (tp5) is at sentence 24: \"After battling with his grief, John wholeheartedly surrendered himself to his newfound spirituality.\"\n",
      "Compared to the fourth turning point, John has come to terms with his grief over Susan's condition and has found a deeper commitment to his spiritual journey. This surrender and commitment to spirituality mark a significant improvement in John's condition, as he finds peace and purpose. Approximate condition: 9\n",
      "\n",
      "### 7. At every turning point, approximate the protagonist's condition in the story as a number from 0 to 10, where 0 is the worst possible condition and 10 is the best possible condition. Put these 5 numbers describing the protagonist's condition in a list in chronological order.\n",
      "Based on the descriptions, the list of John's condition at each turning point is: [4, 5, 7, 3, 9]\n",
      "\n",
      "### 8. Classify the story arc type based on the protagonist's condition list and explain your reasoning.\n",
      "The story arc type can be classified by analyzing the trend of the protagonist's condition throughout the story. Starting at a relatively low point (4), John's condition slightly improves (5) as he begins his spiritual journey, then further improves (7) as he finds community and connection. However, it significantly worsens (3) with Susan's illness before ultimately improving to a high point (9) as John finds peace and purpose through his spirituality. This pattern shows John falling into a dilemma (the financial crisis and personal losses), finding a path towards recovery and growth (through spirituality and community), facing a significant setback (Susan's illness), and ultimately emerging in a better condition than where he started (finding peace and purpose). This cycle of decline, recovery, setback, and final improvement aligns with the **Double Man in a Hole** story arc, where the protagonist faces two cycles of falling into a dilemma and recovering, ultimately ending in a better state.\n",
      "\n",
      "### 9. Determine the story arc type\n",
      "Double Man in a Hole<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# if True:\n",
    "#     from unsloth import FastLanguageModel\n",
    "#     from transformers import AutoTokenizer\n",
    "    \n",
    "#     # Load the base model first\n",
    "#     model = None\n",
    "#     model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#         model_name = \"unsloth/Llama-3.3-70B-Instruct\",\n",
    "#         max_seq_length = max_seq_length,\n",
    "#         dtype = dtype,\n",
    "#         load_in_4bit = load_in_4bit,\n",
    "#         # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "#         cache_dir=cache_dir,\n",
    "#     )\n",
    "    \n",
    "#     # Load the LoRA adapter weights\n",
    "#     model = FastLanguageModel.get_peft_model(model)\n",
    "#     model.load_adapter(\"lora_model\")\n",
    "    \n",
    "#     # Load tokenizer separately\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")\n",
    "    \n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# alpaca_prompt = You MUST copy from above!\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    val_data[0][\"query\"]\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import json\n",
    "\n",
    "from typing import Dict, List, Any, Optional\n",
    "def extract_tp_from_text(text: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract the turning points from the model's generated text if it follows\n",
    "    the $\\boxed{...}$ format containing a JSON structure.\n",
    "    \"\"\"\n",
    "    pattern = r'\\$\\\\boxed\\{(.*?)\\}\\$'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        json_str = match.group(1)\n",
    "        # Convert the string to a standard JSON format\n",
    "        json_str = '{' + json_str.replace('\"', '\"') + '}'\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "### Final Turning Point Locations\n",
    "\n",
    "extract_tp_from_text(\"\"\"Based on the analysis, the final turning point locations in JSON format are:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"\"tp1\"\": 7,\n",
    "  \"\"tp2\"\": 10,\n",
    "  \"\"tp3\"\": 14,\n",
    "  \"\"tp4\"\": 19,\n",
    "  \"\"tp5\"\": 35\n",
    "}\n",
    "```\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
