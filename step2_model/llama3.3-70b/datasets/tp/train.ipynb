{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 8192 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "cache_dir = \"/home/ubuntu/marbles/Projects/narrative-dataset/model_cache-2\"\n",
    "model_name = \"unsloth/Llama-3.3-70B-Instruct\"\n",
    "train_dataset_path = \"./tp_data_train.json\"\n",
    "val_dataset_path = \"./tp_data_val.json\"\n",
    "test_dataset_path = \"./tp_data_test.json\"\n",
    "lora_save_name = \"70b-4bit-lora-r128\"\n",
    "lora_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.48.0.\n",
      "   \\\\   /|    GPU: NVIDIA H100 PCIe. Max memory: 79.097 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 9.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:43<00:00,  5.44s/it]\n",
      "Unsloth 2025.1.5 patched 80 layers with 80 QKV layers, 80 O layers and 80 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_dim, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples: List[Dict[str, Any]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Format prompts by appending EOS token to complete-text.\n",
    "    \"\"\"\n",
    "    output_texts = []\n",
    "    for example in examples:\n",
    "        text = example['complete-text'] + tokenizer.eos_token\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "def load_json_data(json_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load narrative data from a JSON file.\n",
    "    This function wraps multiple JSON objects into a list if they aren't already.\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read().strip()\n",
    "        try:\n",
    "            data = json.loads(content)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "            data = []\n",
    "    \n",
    "    return data\n",
    "\n",
    "train_data = load_json_data(train_dataset_path)\n",
    "formatted_train_texts = formatting_prompts_func(train_data)\n",
    "train_dataset = Dataset.from_list([{\"text\": text} for text in formatted_train_texts])\n",
    "\n",
    "# Load and format validation data  \n",
    "val_data = load_json_data(val_dataset_path)\n",
    "formatted_val_texts = formatting_prompts_func(val_data)\n",
    "val_dataset = Dataset.from_list([{\"text\": text} for text in formatted_val_texts])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15701"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the longest text in the dataset\n",
    "max_len = 0\n",
    "longest_text = \"\"\n",
    "for example in val_dataset:\n",
    "    if len(example[\"text\"]) > max_len:\n",
    "        max_len = len(example[\"text\"])\n",
    "        longest_text = example[\"text\"]\n",
    "len(longest_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/marbles/Projects/miniconda/envs/unsloth_env/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/ubuntu/marbles/Projects/miniconda/envs/unsloth_env/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Map (num_proc=2): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 351/351 [00:02<00:00, 122.99 examples/s]\n",
      "Map (num_proc=2): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 88/88 [00:01<00:00, 47.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 30,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        evaluation_strategy = \"steps\",\n",
    "        eval_steps = 5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 351 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 30\n",
      " \"-____-\"     Number of trainable parameters = 1,656,750,080\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 40:04, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.458400</td>\n",
       "      <td>1.453833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.205700</td>\n",
       "      <td>1.256121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.101700</td>\n",
       "      <td>1.106556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.057400</td>\n",
       "      <td>1.061546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.033400</td>\n",
       "      <td>1.049191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.041800</td>\n",
       "      <td>1.045420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('70b-4bit-lora-r128/tokenizer_config.json',\n",
       " '70b-4bit-lora-r128/special_tokens_map.json',\n",
       " '70b-4bit-lora-r128/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import unsloth_train\n",
    "trainer_stats = unsloth_train(trainer)\n",
    "model.save_pretrained(lora_save_name)  # Local saving\n",
    "tokenizer.save_pretrained(lora_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mval_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m### Response:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "val_dataset[0][\"text\"].split(\"### Response:\\n\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/marbles/Projects/miniconda/envs/unsloth_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"70b-4bit-lora-r128\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>### INSTRUCTIONS\n",
      "        You are a helpful assistant that identifies and explains turning points in a narrative. You are given:\n",
      "        - The story, broken down into numbered sentences.\n",
      "        - The definitions of each of the five turning points (Opportunity, Change of Plans, Point of No Return, Major Setback, Climax).\n",
      "        - Ground truth turning point indices for this story.\n",
      "        \n",
      "        ### TURNING POINT DEFINITIONS\n",
      "        - **Opportunity** ‚Äì Introductory event that occurs after presenting the setting and background of the main characters.\n",
      "        - **Change of Plans** ‚Äì Event where the main goal of the story is defined, starting the main action.\n",
      "        - **Point of No Return** ‚Äì Event that pushes the main character(s) to fully commit to their goal.\n",
      "        - **Major Setback** ‚Äì Event where things fall apart temporarily or permanently.\n",
      "        - **Climax** ‚Äì Final event/resolution of the main story (the \"biggest spoiler\").\n",
      "\n",
      "### INPUT \n",
      "        Story, broken down into numbered sentences:\n",
      "        1) The four friends in the book club meet in person after COVID-19 forced them to switch their monthly meetings to Zoom calls.\n",
      "2) During that time, Sharon retires as a judge, Carol's husband Bruce suffers a heart attack, and Carol has to close her Italian restaurant.\n",
      "3) Vivian, who has been confined with Arthur, reveals their engagement once the confinement is over and the club can meet again in person.\n",
      "4) Carol reminds them of a trip to Italy they did not go on years previously so, now that the travel ban is lifted, she proposes the four friends take a bachelorette trip to Italy culminating in the final destination in Tuscany.\n",
      "5) At first, the three friends resist, but Arthur encourages Vivian to go, Sharon's cat dies and Mitchell also convinces Diane.\n",
      "6) The four thoroughly enjoy their stay in Rome, sightseeing, man watching, being admired by the locals and shopping for Vivian's wedding gown.\n",
      "7) A street artist assures them a trip to Venice is a must for tourists, so they take a train.\n",
      "8) At the station, they hand over their luggage to the ‚Äúporters‚Äù at the station, not realizing they are thieves.\n",
      "9) Diane loses the urn containing her deceased husband Harry‚Äôs ashes, which she has illegally brought into the country, hidden in her suitcase.\n",
      "10) Sharon calls the police chief who attends them lazy for his treatment of the theft.\n",
      "11) Carol points out that their experience is like what happened to a character in Paolo Coelho's The Alchemist, so they should treat it as part of their adventure too.\n",
      "12) Vivian‚Äôs hotel connections get them nice accommodations in Venice.\n",
      "13) In a bar, Sharon meets Ousmane, a retired philosophy professor who invites the foursome to dinner.\n",
      "14) The venue turns out to be famed Chef Gianni‚Äôs cooking school, and he and Carol are blown away by the surprise of seeing each other again.\n",
      "15) After an evening of eating, drinking and dancing, Sharon and Ousmane leave together and Carol visits Gianni's kitchen.\n",
      "16) There is a spark between Carol and Gianni, which they channel into kneading dough.\n",
      "17) Sharon and Ousmane have a passionate encounter on his boat, and are found by the same police chief she called lazy.\n",
      "18) The foursome rent a car to drive to Tuscany, as Diane especially fears thieves on the train.\n",
      "19) On the road, Carol panics as Gianni has shared photos of their cooking session, labeling them in his broken English.\n",
      "20) Realizing Bruce will see them as they will also go to her iPad, she frantically calls him.\n",
      "21) Her friends, giving her tough love, point out she is stifling both of them and not enjoying life.\n",
      "22) They have a flat tire on the secondary road without a spare, forcing them to hitchhike.\n",
      "23) Diane reveals that they need to be in Tuscany in the morning as Arthur is surprising Vivian with an intimate destination wedding there.\n",
      "24) When a hunky policeman happens by, Vivian thinks he is a male stripper so she tries to pull off his clothes.\n",
      "25) That lands them in the local jail.\n",
      "26) There, both Vivian and Diane get tough love and honest feedback, before Sharon goads the same prickly police chief into getting them to Vivian‚Äôs wedding via helicopter.\n",
      "27) As Diane‚Äôs urn was recovered, he lets her scatter her husband's ashes en route.\n",
      "28) Arthur and Mitchell (his best man, and Diane‚Äôs boyfriend) are waiting, and Carol is surprised by Bruce's appearance.\n",
      "29) Judge Sharon officiates the wedding.\n",
      "30) Vivian hedges about going through with the wedding, so Arthur pledges his love and asks her to not marry him.\n",
      "31) When Mitchell asks Diane to marry him, she accepts and Sharon marries them on the spot.\n",
      "32) The four friends realize their current book, The Alchemist by Paolo Coelho, predicted their lives more than they expected it to.\n",
      "\n",
      "        Please use the provided definitions and the sentence indices to produce an explanation **why** each sentence index corresponds to the turning point category. End with a concise summary that reiterates the turning points in order.\n",
      "\n",
      "        ### RESPONSE\n",
      "        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ### Task 1: Identify the Protagonist or Central Group of Characters\n",
      "The central group of characters in the story consists of four friends: Sharon, Carol, Vivian, and Diane. They are the main focus of the narrative, and the story revolves around their experiences, relationships, and personal growth.\n",
      "\n",
      "### Task 2: Summarize Each Major Event and Explore How Those Events Change the Protagonist's Condition\n",
      "1. **Introduction and Background**: The story begins with the four friends meeting in person after a period of virtual meetings due to COVID-19. This setting establishes their friendship and individual backgrounds, including significant life changes such as Sharon's retirement, Carol's husband's heart attack, and Vivian's engagement.\n",
      "2. **The Trip Proposal**: Carol suggests a bachelorette trip to Italy, which initially meets resistance but eventually gains traction as a way for them to reconnect and celebrate Vivian's upcoming wedding. This proposal sets the stage for their adventures and personal reflections.\n",
      "3. **Enjoyment and Theft in Italy**: The friends enjoy their time in Rome, but their experience takes a turn when they fall victim to theft in a train station, losing their luggage and, more significantly, Diane's urn containing her husband's ashes. This event introduces conflict and sets them on a path of unexpected adventures and self-discovery.\n",
      "4. **New Connections and Reflections**: In Venice, they meet new people, including a street artist, a police chief, and a retired philosophy professor, Ousmane. These encounters lead to personal connections, especially for Sharon and Carol, who find themselves in romantic and introspective situations. These connections prompt them to reflect on their lives, desires, and the true meaning of their trip.\n",
      "5. **Challenges and Growth**: The friends face several challenges, including a flat tire, a misunderstanding with a policeman, and a stint in jail. However, these challenges also lead to moments of growth, particularly for Vivian and Diane, who receive tough love and honest feedback. This growth is crucial as they approach the climax of their trip and their personal journeys.\n",
      "6. **The Wedding and Resolutions**: The story culminates in Vivian's wedding, which becomes a pivotal moment for all four friends. The event leads to unexpected turns, including Vivian's hesitation and Arthur's heartfelt pledge, followed by Mitchell's proposal to Diane. The story concludes with a sense of resolution and new beginnings for the characters, as they reflect on how their lives have been predicted and influenced by the book they've been reading, \"The Alchemist.\"\n",
      "\n",
      "### Task 3: Detailed Explanation of Each Ground Truth Turning Point\n",
      "1. **Opportunity (tp1): Sentence 4.0**\n",
      "   - This sentence marks the opportunity for the friends to embark on a new adventure together, proposed by Carol as a bachelorette trip to Italy. It's an introductory event that sets the main action in motion, aligning with the definition of an opportunity as it presents a chance for the characters to explore new experiences, reconnect, and potentially find personal growth.\n",
      "\n",
      "2. **Change of Plans (tp2): Sentence 8.0**\n",
      "   - At the train station, the friends hand over their luggage to thieves, not realizing their mistake. This event changes the course of their trip, introducing unexpected challenges and adventures. It defines the main action of the story, as the friends must now navigate these unforeseen circumstances, making it a change of plans.\n",
      "\n",
      "3. **Point of No Return (tp3): Sentence 17.0**\n",
      "   - Sharon and Ousmane's passionate encounter, followed by their run-in with the police chief, marks a point where the characters are fully committed to their personal explorations and adventures. This event signifies a deeper level of engagement with their desires and a willingness to take risks, pushing them beyond a point of return to their previous states, both individually and as a group.\n",
      "\n",
      "4. **Major Setback (tp4): Sentence 25.0**\n",
      "   - The friends' attempt to hitchhike, followed by Vivian's misunderstanding of a policeman as a male stripper, lands them in jail. This event is a major setback as it temporarily derails their plans, causes them distress, and forces them to confront their actions and decisions. It's a point where things fall apart, requiring them to regroup and find a new path forward.\n",
      "\n",
      "5. **Climax (tp5): Sentence 30.0**\n",
      "   - Judge Sharon officiating Vivian's wedding is the climax of the story. It's the final event that resolves the main storyline, bringing together the various threads of personal growth, relationships, and adventure. The climax is not just the wedding itself but the culmination of the friends' journeys, marked by Vivian's hesitation and the subsequent proposals, leading to a resolution that changes their lives forever.\n",
      "\n",
      "### Task 4: Conclusion and Summary of Turning Points\n",
      "In summary, the turning points in the story are:\n",
      "- **Opportunity (tp1)**: Sentence 4.0 - Carol proposes a bachelorette trip to Italy.\n",
      "- **Change of Plans (tp2)**: Sentence 8.0 - The friends fall victim to theft at the train station.\n",
      "- **Point of No Return (tp3)**: Sentence 17.0 - Sharon and Ousmane's encounter and subsequent run-in with the police.\n",
      "- **Major Setback (tp4)**: Sentence 25.0 - The friends are jailed after a misunderstanding with a policeman.\n",
      "- **Climax (tp5)**: Sentence 30.0 - Vivian's wedding, officiated by Judge Sharon, marking the resolution of the main storyline and the characters' personal journeys.\n",
      "\n",
      "These turning points signify critical changes in the protagonists' conditions, guiding the narrative through moments of adventure, personal growth, challenge, and ultimately, resolution.Final Turning Points: \n",
      "{'tp1': 4.0, 'tp2': 8.0, 'tp3': 17.0, 'tp4': 25.0, 'tp5': 30.0}<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# if True:\n",
    "#     from unsloth import FastLanguageModel\n",
    "#     from transformers import AutoTokenizer\n",
    "    \n",
    "#     # Load the base model first\n",
    "#     model = None\n",
    "#     model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#         model_name = \"unsloth/Llama-3.3-70B-Instruct\",\n",
    "#         max_seq_length = max_seq_length,\n",
    "#         dtype = dtype,\n",
    "#         load_in_4bit = load_in_4bit,\n",
    "#         # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "#         cache_dir=cache_dir,\n",
    "#     )\n",
    "    \n",
    "#     # Load the LoRA adapter weights\n",
    "#     model = FastLanguageModel.get_peft_model(model)\n",
    "#     model.load_adapter(\"lora_model\")\n",
    "    \n",
    "#     # Load tokenizer separately\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")\n",
    "    \n",
    "#     FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# alpaca_prompt = You MUST copy from above!\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    val_data[0][\"query\"]\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import json\n",
    "\n",
    "from typing import Dict, List, Any, Optional\n",
    "def extract_tp_from_text(text: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract the turning points from the model's generated text if it follows\n",
    "    the $\\boxed{...}$ format containing a JSON structure.\n",
    "    \"\"\"\n",
    "    pattern = r'\\$\\\\boxed\\{(.*?)\\}\\$'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        json_str = match.group(1)\n",
    "        # Convert the string to a standard JSON format\n",
    "        json_str = '{' + json_str.replace('\"', '\"') + '}'\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "### Final Turning Point Locations\n",
    "\n",
    "extract_tp_from_text(\"\"\"Based on the analysis, the final turning point locations in JSON format are:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"\"tp1\"\": 7,\n",
    "  \"\"tp2\"\": 10,\n",
    "  \"\"tp3\"\": 14,\n",
    "  \"\"tp4\"\": 19,\n",
    "  \"\"tp5\"\": 35\n",
    "}\n",
    "```\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
