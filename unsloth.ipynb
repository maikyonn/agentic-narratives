{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/marbles/Projects/miniconda/envs/unsloth_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Your Flash Attention 2 installation seems to be broken?\n",
      "A possible explanation is you have a new CUDA version which isn't\n",
      "yet compatible with FA2? Please file a ticket to Unsloth or FA2.\n",
      "We shall now use Xformers instead, which does not have any performance hits!\n",
      "We found this negligible impact by benchmarking on 1x A100.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 8192 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "cache_dir = \"./model_cache-2\"\n",
    "model_name = \"unsloth/Llama-3.3-70B-Instruct\"\n",
    "train_dataset_path = \"step2_model/llama3.3-70b/datasets/tp/tp_data_train.json\"\n",
    "val_dataset_path = \"step2_model/llama3.3-70b/datasets/tp/tp_data_val.json\"\n",
    "test_dataset_path = \"./datasets/test.json\"\n",
    "lora_save_name = \"70b-4bit-lora-r128\"\n",
    "lora_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.48.0.\n",
      "   \\\\   /|    GPU: NVIDIA H100 PCIe. Max memory: 79.097 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 9.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:32<00:00,  4.11s/it]\n",
      "Unsloth 2025.1.5 patched 80 layers with 80 QKV layers, 80 O layers and 80 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    cache_dir=cache_dir,\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_dim, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from training dataset:\n",
      "--------------------------------------------------\n",
      "### INSTRUCTIONS\n",
      "        You are a helpful assistant that identifies and explains turning points in a narrative. You are given:\n",
      "        - The story, broken down into numbered sentences.\n",
      "        - The definitions of each of the five turning points (Opportunity, Change of Plans, Point of No Return, Major Setback, Climax).\n",
      "        - Ground truth turning point indices for this story.\n",
      "        \n",
      "        ### TURNING POINT DEFINITIONS\n",
      "        - **Opportunity** â€“ Introductory event that occurs after presenting the setting and background of the main characters.\n",
      "        - **Change of Plans** â€“ Event where the main goal of the story is defined, starting the main action.\n",
      "        - **Point of No Return** â€“ Event that pushes the main character(s) to fully commit to their goal.\n",
      "        - **Major Setback** â€“ Event where things fall apart temporarily or permanently.\n",
      "        - **Climax** â€“ Final event/resolution of the main story (the \"biggest spoiler\").\n",
      "\n",
      "### Input:\n",
      "Story, broken down into numbered sentences:\n",
      "            1) As the formidable grip of the Great Depression continues to take its toll, the teeming streets of 1930s New York City become the backdrop to a compelling narrative of resilience and survival against all odds.\n",
      "2) Introducing our protagonists, Jack Dempsey, an experienced and good-natured young man, along with his younger sister, Mary, who are orphaned by their father's tragic suicide following the loss of his prosperous coal business.\n",
      "3) Left to fend for themselves, they wrestle with the relentless poverty, surviving on scarce food and any charity they receive.\n",
      "4) The continuous burden of poverty pushes Jack to devise a daring plan to rob a midtown bank, hoping that such a dangerous act could offer them a brief respite from their unending ordeals.\n",
      "5) Opening up to Mary about his perilous scheme, her alarm and disapproval at his misguided solution spur a tense familial conflict that fuels the central narrative of the film.\n",
      "6) Unmoved by Mary's objections, Jack recruits his trusted accomplices from the crime-ridden neighborhood of Little Italy - Sal, a seasoned pickpocket with striking silver hair; Lily, a skillful lock-picker, and Eddie, a fast-talking impersonator.\n",
      "7) Creating moral tension, Mary, as the conscience of the narrative, vehemently urges Jack to abandon his dangerous plan, suggesting they rely on manual labor instead.\n",
      "8) Their clash of beliefs leads to a deeply emotional confrontation, highlighting their stark ideological differences and resulting in a painful rift between the siblings.\n",
      "9) Unshaken by Mary's opposition, Jack and his associates begin the meticulous preparation for the heist, acquiring various locksmithing tools, familiarizing themselves with the layout of the Midtown Bank, and closely monitoring the routines of its security staff.\n",
      "10) Tormented by the fear of losing her brother, Mary, betraying Jack's trust, tips off the local police about his planned robbery, securing a promise of her brother's safety in return.\n",
      "11) As the momentous event draws near, a palpable tension engulfs the city as Jack and his cohorts brace themselves for their moment of reckoning.\n",
      "12) Implementing their intricately planned operation, they confront and tactfully navigate through several unexpected obstacles.\n",
      "13) The tension peaks as they expertly evade security cameras, disarm alarm systems, cleverly manoeuvre through a host of traps, and distract security personnel to finally access the bank's treasury.\n",
      "14) The successful heist prompts a euphoric burst of joy amongst the downtrodden gang, their spirits momentarily soaring higher than New York's towering structures.\n",
      "15) However, the fleeting victory is quickly overshadowed as they find themselves encircled by heavily armed police officers, ensuing a high-stakes chase through the city's grimy, labyrinthine streets.\n",
      "16) In this unexpected showdown, a painful realization dawns upon Jack â€“ they have been betrayed.\n",
      "17) Nonetheless, in a thrilling twist, Jack unveils his trump card â€“ a covert passage leading straight into the city's expansive sewage network.\n",
      "18) A heart-stopping chase ensues as the gang sprints for the secret exit, with Jack bravely holding the police at bay, buying his comrades some precious time.\n",
      "19) They manage a hair-raising escape through the cityâ€™s underground network, leaving behind the echo of the blaring sirens and frantic shouts.\n",
      "20) Finding a secure hiding spot, Jack finally reveals his true motives: the stolen money is meant to provide relief for the cityâ€™s impoverished.\n",
      "21) As they distribute their unlawful gains, they bring a glimmer of hope to New York's lowest rungs, offering a momentary respite from the grim everyday existence.\n",
      "22) In a final act of redemption, Jack forgives Mary for her betrayal and shares a generous portion of the stolen funds with her.\n",
      "23) His accomplices, who've weathered numerous hardships, find themselves finally enjoying a life free from the terror of poverty and hunger.\n",
      "24) Observing from a safe distance, Jack watches his former partners and sister basking in their newfound security and happiness.\n",
      "25) Leaving them behind, Jack disappears into the shadowy alleyways of the city, resolved to start afresh alone.\n",
      "26) As the credits begin to roll, the viewers are left contemplating his actions â€“ was his unconventional method simply a desperate bid for survival, or were they driven by a selfless desire to alleviate the suffering of others?\n",
      "27) As the credits begin to roll, the narrative leaves us pondering over the paradox of Jackâ€™s actions - how even the darkest deeds, driven by an unwavering sense of humanity, can bring a beacon of light into the lives of those barely surviving.\n",
      "\n",
      "        Please use the provided definitions and the sentence indices to produce an explanation **why** each sentence index corresponds to the turning point category. End with a concise summary that reiterates the turning points in order.\n",
      "\n",
      "        ### RESPONSE\n",
      "        ### Task 1: Identify the Protagonist or Central Group of Characters\n",
      "The central characters in the story are Jack Dempsey and his younger sister, Mary. Jack is the primary protagonist, as the narrative revolves around his actions, decisions, and the consequences thereof. Mary plays a crucial role as well, often acting as the conscience of the story and influencing Jack's choices.\n",
      "\n",
      "### Task 2: Summarize Each Major Event and Explore How Those Events Change the Protagonist's Condition\n",
      "1. **Introduction to the Setting and Characters**: The story begins by setting the scene in 1930s New York City, amidst the Great Depression, and introduces Jack and Mary, who have been orphaned and are struggling to survive.\n",
      "2. **The Plan to Rob a Bank**: Jack devises a plan to rob a bank to temporarily alleviate their poverty, introducing conflict and setting the stage for the main action.\n",
      "3. **Recruitment of Accomplices**: Jack recruits Sal, Lily, and Eddie for the heist, marking a point of no return in their commitment to the plan.\n",
      "4. **Mary's Opposition and Betrayal**: Mary's disapproval and subsequent betrayal by tipping off the police create tension and raise the stakes, leading to a significant change in the protagonist's condition.\n",
      "5. **The Heist and Chase**: The successful bank robbery followed by a high-stakes chase introduces a major setback and tests the characters' resolve and ingenuity.\n",
      "6. **Escape and Revelation of Motives**: The gang's escape through the sewage network and Jack's revelation that the stolen money is to help the impoverished change the protagonist's condition by showing a shift towards redemption.\n",
      "7. **Redemption and New Beginnings**: The distribution of the stolen money, Jack's forgiveness of Mary, and the improvement in the lives of those around him mark a climax in the story, leading to a conclusion where the protagonist seeks a new beginning.\n",
      "\n",
      "### Task 3: Detailed Explanation of Each Ground Truth Turning Point\n",
      "1. **Opportunity (tp1): Sentence 4.0** - This sentence qualifies as the Opportunity turning point because it introduces the event that sets the main story in motion. Jack's decision to rob a bank is a direct response to their desperate situation, offering a potential way out of their poverty. This marks the beginning of the main action and sets the stage for the characters' journey.\n",
      "\n",
      "2. **Change of Plans (tp2): Sentence 7.0** - This sentence represents the Change of Plans turning point. Here, Mary suggests relying on manual labor instead of the bank heist, which could have altered the course of their actions. However, Jack's commitment to the heist signifies that the main goal of the story (the bank robbery) is now defined, and the characters are moving towards its execution.\n",
      "\n",
      "3. **Point of No Return (tp3): Sentence 10.0** - Mary's decision to tip off the police about the planned robbery marks the Point of No Return. This action commits the characters to a path from which there is no easy return. For Jack and his accomplices, the risk of being caught is now higher, and for Mary, the betrayal of her brother's trust has significant emotional implications, making this a point of commitment to their respective paths.\n",
      "\n",
      "4. **Major Setback (tp4): Sentence 15.0** - The event where the gang finds themselves encircled by heavily armed police officers after the heist is a Major Setback. This is the moment when things fall apart, and the characters face a significant obstacle that threatens their survival and the success of their plan. It tests their ability to respond under pressure and forces them to improvise to survive.\n",
      "\n",
      "5. **Climax (tp5): Sentence 21.0** - The distribution of the stolen money to provide relief for the city's impoverished marks the Climax of the story. This event is the culmination of the main action and resolves the central conflict. It reveals Jack's true motives and shows the positive impact of his actions, leading to a moment of redemption and change for the characters involved.\n",
      "\n",
      "### Task 4: Conclusion and Summary of Turning Points\n",
      "In summary, the turning points in the story, in order, are:\n",
      "- **Opportunity (tp1)**: Sentence 4.0 - Jack decides to rob a bank to alleviate their poverty.\n",
      "- **Change of Plans (tp2)**: Sentence 7.0 - Mary suggests manual labor, but Jack remains committed to the heist.\n",
      "- **Point of No Return (tp3)**: Sentence 10.0 - Mary tips off the police, committing the characters to a risky path.\n",
      "- **Major Setback (tp4)**: Sentence 15.0 - The gang is encircled by police after the heist, threatening their survival.\n",
      "- **Climax (tp5)**: Sentence 21.0 - The distribution of stolen money provides relief to the impoverished, marking a point of redemption.\n",
      "\n",
      "These turning points signify critical changes in the protagonist's condition, driving the narrative forward and shaping the story's outcome.\n",
      "        \n",
      "### Response:\n",
      "### Task 1: Identify the Protagonist or Central Group of Characters\n",
      "The central characters in the story are Jack Dempsey and his younger sister, Mary. Jack is the primary protagonist, as the narrative revolves around his actions, decisions, and the consequences thereof. Mary plays a crucial role as well, often acting as the conscience of the story and influencing Jack's choices.\n",
      "\n",
      "### Task 2: Summarize Each Major Event and Explore How Those Events Change the Protagonist's Condition\n",
      "1. **Introduction to the Setting and Characters**: The story begins by setting the scene in 1930s New York City, amidst the Great Depression, and introduces Jack and Mary, who have been orphaned and are struggling to survive.\n",
      "2. **The Plan to Rob a Bank**: Jack devises a plan to rob a bank to temporarily alleviate their poverty, introducing conflict and setting the stage for the main action.\n",
      "3. **Recruitment of Accomplices**: Jack recruits Sal, Lily, and Eddie for the heist, marking a point of no return in their commitment to the plan.\n",
      "4. **Mary's Opposition and Betrayal**: Mary's disapproval and subsequent betrayal by tipping off the police create tension and raise the stakes, leading to a significant change in the protagonist's condition.\n",
      "5. **The Heist and Chase**: The successful bank robbery followed by a high-stakes chase introduces a major setback and tests the characters' resolve and ingenuity.\n",
      "6. **Escape and Revelation of Motives**: The gang's escape through the sewage network and Jack's revelation that the stolen money is to help the impoverished change the protagonist's condition by showing a shift towards redemption.\n",
      "7. **Redemption and New Beginnings**: The distribution of the stolen money, Jack's forgiveness of Mary, and the improvement in the lives of those around him mark a climax in the story, leading to a conclusion where the protagonist seeks a new beginning.\n",
      "\n",
      "### Task 3: Detailed Explanation of Each Ground Truth Turning Point\n",
      "1. **Opportunity (tp1): Sentence 4.0** - This sentence qualifies as the Opportunity turning point because it introduces the event that sets the main story in motion. Jack's decision to rob a bank is a direct response to their desperate situation, offering a potential way out of their poverty. This marks the beginning of the main action and sets the stage for the characters' journey.\n",
      "\n",
      "2. **Change of Plans (tp2): Sentence 7.0** - This sentence represents the Change of Plans turning point. Here, Mary suggests relying on manual labor instead of the bank heist, which could have altered the course of their actions. However, Jack's commitment to the heist signifies that the main goal of the story (the bank robbery) is now defined, and the characters are moving towards its execution.\n",
      "\n",
      "3. **Point of No Return (tp3): Sentence 10.0** - Mary's decision to tip off the police about the planned robbery marks the Point of No Return. This action commits the characters to a path from which there is no easy return. For Jack and his accomplices, the risk of being caught is now higher, and for Mary, the betrayal of her brother's trust has significant emotional implications, making this a point of commitment to their respective paths.\n",
      "\n",
      "4. **Major Setback (tp4): Sentence 15.0** - The event where the gang finds themselves encircled by heavily armed police officers after the heist is a Major Setback. This is the moment when things fall apart, and the characters face a significant obstacle that threatens their survival and the success of their plan. It tests their ability to respond under pressure and forces them to improvise to survive.\n",
      "\n",
      "5. **Climax (tp5): Sentence 21.0** - The distribution of the stolen money to provide relief for the city's impoverished marks the Climax of the story. This event is the culmination of the main action and resolves the central conflict. It reveals Jack's true motives and shows the positive impact of his actions, leading to a moment of redemption and change for the characters involved.\n",
      "\n",
      "### Task 4: Conclusion and Summary of Turning Points\n",
      "In summary, the turning points in the story, in order, are:\n",
      "- **Opportunity (tp1)**: Sentence 4.0 - Jack decides to rob a bank to alleviate their poverty.\n",
      "- **Change of Plans (tp2)**: Sentence 7.0 - Mary suggests manual labor, but Jack remains committed to the heist.\n",
      "- **Point of No Return (tp3)**: Sentence 10.0 - Mary tips off the police, committing the characters to a risky path.\n",
      "- **Major Setback (tp4)**: Sentence 15.0 - The gang is encircled by police after the heist, threatening their survival.\n",
      "- **Climax (tp5)**: Sentence 21.0 - The distribution of stolen money provides relief to the impoverished, marking a point of redemption.\n",
      "\n",
      "These turning points signify critical changes in the protagonist's condition, driving the narrative forward and shaping the story's outcome.<|eot_id|>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pprint\n",
    "\n",
    "def load_json_data(json_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load narrative data from a JSON file.\n",
    "    This function wraps multiple JSON objects into a list if they aren't already.\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read().strip()\n",
    "        try:\n",
    "            data = json.loads(content)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "            data = []\n",
    "    \n",
    "    return data\n",
    "\n",
    "def formatting_prompts_func(examples: List[Dict[str, Any]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Format prompts by combining instruction, input, and output from each example.\n",
    "    \"\"\"\n",
    "    output_texts = []\n",
    "    for example in examples:\n",
    "        text = (\n",
    "            f\"{example['instruction']}\\n\\n\"\n",
    "            f\"### Input:\\n{example['input']}\\n\" \n",
    "            f\"### Response:\\n{example['output']}\"\n",
    "        ) + tokenizer.eos_token  # Ensure `tokenizer` is defined\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "# Load and format training data\n",
    "train_data = load_json_data(train_dataset_path)\n",
    "formatted_train_texts = formatting_prompts_func(train_data)\n",
    "train_dataset = Dataset.from_list([{\"text\": text} for text in formatted_train_texts])\n",
    "\n",
    "# Load and format validation data\n",
    "val_data = load_json_data(val_dataset_path)\n",
    "formatted_val_texts = formatting_prompts_func(val_data)\n",
    "val_dataset = Dataset.from_list([{\"text\": text} for text in formatted_val_texts])\n",
    "\n",
    "# Print a sample from the training dataset\n",
    "print(\"Sample from training dataset:\")\n",
    "print(\"-\" * 50)\n",
    "print(train_dataset[0][\"text\"])\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/marbles/Projects/miniconda/envs/unsloth_env/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/ubuntu/marbles/Projects/miniconda/envs/unsloth_env/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 351/351 [00:03<00:00, 113.55 examples/s]\n",
      "Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:01<00:00, 48.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 50,\n",
    "        learning_rate = 2e-4,\n",
    "        dataloader_num_workers = 32,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"step2_model/llama3.3-70b/datasets/tp/outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        evaluation_strategy = \"steps\",\n",
    "        eval_steps = 10,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 351 | Num Epochs = 2\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 50\n",
      " \"-____-\"     Number of trainable parameters = 414,187,520\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4/50 01:30 < 34:38, 0.02 it/s, Epoch 0.07/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import unsloth_train\n",
    "trainer_stats = unsloth_train(trainer)\n",
    "model.save_pretrained(lora_save_name)  # Local saving\n",
    "tokenizer.save_pretrained(lora_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### Response:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset[0][\"text\"].split(\"### Response:\\n\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the turning points and their sentence number in this narrative and explain your reasoning step-by-step. Then provide the final turning point locations clearly in a json format, like {\"tp1\": ##.#, \"tp2\": ##.#, \"tp3\": ##.#, \"tp4\": ##.#, \"tp5\": ##.#}.\n",
      "\n",
      "    ### TURNING POINT DEFINITIONS\n",
      "    1. **Opportunity** â€“ Introductory event that occurs after presenting the setting and background of the main characters.\n",
      "    2. **Change of Plans** â€“ Event where the main goal of the story is defined, starting the main action.\n",
      "    3. **Point of No Return** â€“ Event that pushes the main character(s) to fully commit to their goal.\n",
      "    4. **Major Setback** â€“ Event where things fall apart temporarily or permanently.\n",
      "    5. **Climax** â€“ Final event/resolution of the main story (the \"biggest spoiler\").\n",
      "    \n",
      "\n",
      "### Synopsis:\n",
      "1. In Havana, Cuba, AndrÃ©s HernÃ¡ndez and his kinkajou Vivo play music together in the plaza.\n",
      "2. One day after their show, AndrÃ©s receives a letter from his old friend, Marta Sandoval, informing him that she is retiring from her music career.\n",
      "3. The letter offers a chance to reconnect in Miami, at the Mambo Cabana and for AndrÃ©s to finally confess his love for Marta through a song he wrote just for her.\n",
      "4. Vivo, happy with their life in Cuba, is reluctant to help AndrÃ©s.\n",
      "5. The next morning, AndrÃ©s dies in his sleep and that night, a funeral service is held in the plaza, with AndrÃ©s' niece-in-law Rosa and her daughter Gabi attending before they head back to their home in Key West, Florida.\n",
      "6. Ashamed of his earlier reluctance, Vivo vows himself to get Marta to hear AndrÃ©s' song.\n",
      "7. He stows away to Key West with Gabi and Rosa in their luggage.\n",
      "8. Gabi finds Vivo and agrees to help him deliver AndrÃ©s' song to Marta.\n",
      "9. Under the guise of attending a cookie sale in town, Gabi and Vivo purchase bus tickets to get to Marta's show, but they are stopped by the Sand Dollars, a girl scout troop who wants Vivo vaccinated.\n",
      "10. Gabi and Vivo escape from them, but miss the bus.\n",
      "11. They end up in the Everglades and are separated by a heavy rain storm, losing the song.\n",
      "12. While searching for Gabi, Vivo comes across a roseate spoonbill named Dancarino, who is unsuccessful in finding love with one of his own.\n",
      "13. With Vivo's help, he is able to win the heart of Valentina.\n",
      "14. The two later rescue Vivo from a Burmese python named Lutador.\n",
      "15. Meanwhile, Gabi discovers that she was followed by the Sand Dollars on a boat and that they have AndrÃ©s' song, keeping it from her until she leads them to Vivo.\n",
      "16. When the girls are attacked by Lutador, Vivo saves them, but the song is destroyed in the process.\n",
      "17. Devastated, Vivo considers returning to Cuba until he realizes he and Gabi can recreate the song, as he knows the melody and Gabi knows the lyrics.\n",
      "18. Together, they make it to Miami and search for Marta, who has learned of AndrÃ©s' death and refuses to go on stage.\n",
      "19. Gabi and Vivo sneak inside the Mambo Cabana, but Gabi is unable to enter and tells Vivo to go on without her.\n",
      "20. She is soon caught by security and her furious mother.\n",
      "21. Vivo finds a mourning Marta, who recognizes him from AndrÃ©s' obituary photo, and delivers the song.\n",
      "22. Touched by the song, Marta is revitalized and decides to go on stage.\n",
      "23. Vivo then locates Gabi and Rosa, who are arguing while driving back home.\n",
      "24. Gabi confesses she decided to help Vivo because she misses her deceased father Carlos.\n",
      "25. Deeply moved, Rosa reassures her daughter, and drives Gabi and Vivo back to the concert just in time to hear Marta play AndrÃ©s' song.\n",
      "26. Vivo decides to stay in Florida with Gabi and Rosa.\n",
      "27. Gabi and Vivo put on their own show in the city with Marta, entertaining the crowd.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Response:\n",
      "### Review of the Story\n",
      "The story revolves around Vivo, a kinkajou who plays music with his owner AndrÃ©s in Havana, Cuba. After AndrÃ©s' death, Vivo decides to fulfill AndrÃ©s' dream of confessing his love to Marta Sandoval through a song. Vivo embarks on a journey from Cuba to Miami, facing various challenges and making new friends along the way, including Gabi, who helps him in his quest.\n",
      "\n",
      "### Summary of Each Segment and Character Development\n",
      "- **Vivo**: Begins as a reluctant participant in AndrÃ©s' plans but becomes the main character driven by guilt and a desire to fulfill AndrÃ©s' last wish. He faces numerous challenges, makes new friends, and learns about perseverance and friendship.\n",
      "- **Gabi**: Initially hesitant but then fully invests in helping Vivo. She learns valuable lessons about friendship, loyalty, and dealing with personal loss.\n",
      "- **Marta**: Introduced as the object of AndrÃ©s' affections, she is later revealed to be grieving AndrÃ©s' death. Through Vivo's efforts, she finds solace and decides to continue her music career.\n",
      "\n",
      "### Chain-of-Thought Explanation for Each Turning Point\n",
      "\n",
      "#### Opportunity (tp1): Sentence 3\n",
      "The opportunity turning point is defined as an introductory event that occurs after presenting the setting and background of the main characters. In sentence 3, AndrÃ©s receives a letter from Marta Sandoval, informing him about her retirement and inviting him to reconnect in Miami. This event sets the stage for the main action of the story by introducing a potential love interest for AndrÃ©s and a chance for him to confess his feelings through a song he wrote specifically for Marta. It presents an opportunity for AndrÃ©s to fulfill his personal goal, which later becomes Vivo's mission after AndrÃ©s' death.\n",
      "\n",
      "#### Change of Plans (tp2): Sentence 6\n",
      "The change of plans turning point is where the main goal of the story is defined, starting the main action. In sentence 6, Vivo, feeling ashamed of his earlier reluctance to help AndrÃ©s, vows to get Marta to hear AndrÃ©s' song after AndrÃ©s' death. This marks a significant shift in the story as Vivo takes on the responsibility of fulfilling AndrÃ©s' dream, changing the direction of the narrative from a simple story of unrequited love to a journey of perseverance and friendship.\n",
      "\n",
      "#### Point of No Return (tp3): Sentence 11\n",
      "The point of no return is an event that pushes the main character(s) to fully commit to their goal. In sentence 11, Gabi and Vivo end up in the Everglades and are separated by a heavy rain storm, losing the song. This event signifies a point where Vivo and Gabi face significant obstacles that test their commitment to their mission. Despite the setbacks, they continue their journey, indicating a full commitment to delivering the song to Marta, even if it means recreating it.\n",
      "\n",
      "#### Major Setback (tp4): Sentence 15\n",
      "A major setback is an event where things fall apart temporarily or permanently. In sentence 15, the Sand Dollars attack Gabi and Vivo, leading to the destruction of the song. This event represents a significant setback as it seems to eliminate the possibility of fulfilling AndrÃ©s' wish through the original song. It forces Vivo and Gabi to reconsider their approach and find an alternative solution, making it a critical point in the story where their resolve is tested.\n",
      "\n",
      "#### Climax (tp5): Sentence 21\n",
      "The climax is the final event or resolution of the main story. In sentence 21, Vivo delivers the recreated song to Marta, who is touched by it and decides to go on stage. This moment marks the culmination of Vivo's journey and the fulfillment of AndrÃ©s' dream. It's the most intense and critical point in the story as it resolves the main conflict (delivering the song to Marta) and sets the stage for the story's resolution, including the reunion of Gabi with her mother and Vivo finding a new home.\n",
      "\n",
      "### Conclusion\n",
      "In summary, the turning points in the story are:\n",
      "1. **Opportunity (tp1)**: Sentence 3 - AndrÃ©s receives an invitation from Marta, setting the stage for the main action.\n",
      "2. **Change of Plans (tp2)**: Sentence 6 - Vivo decides to fulfill AndrÃ©s' dream after his death.\n",
      "3. **Point of No Return (tp3)**: Sentence 11 - Gabi and Vivo face significant obstacles in the Everglades.\n",
      "4. **Major Setback (tp4)**: Sentence 15 - The song is destroyed during an attack by the Sand Dollars.\n",
      "5. **Climax (tp5)**: Sentence 21 - Vivo delivers the song to Marta, fulfilling AndrÃ©s' dream and resolving the main conflict.\n",
      "\n",
      "These turning points guide the narrative arc of the story, shaping the characters' journeys and the overall plot progression.\n",
      "\n",
      "Final Turning Points: {'tp1': 3.0, 'tp2': 6.0, 'tp3': 11.0, 'tp4': 15.0, 'tp5': 21.0}<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# alpaca_prompt = You MUST copy from above!\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    dataset[0][\"text\"].split(\"### Response:\\n\")[0]\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.48.0.\n",
      "   \\\\   /|    GPU: NVIDIA H100 PCIe. Max memory: 79.097 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 9.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load the base model first\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munsloth/Llama-3.3-70B-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Load the LoRA adapter weights\u001b[39;00m\n\u001b[1;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m FastLanguageModel\u001b[38;5;241m.\u001b[39mget_peft_model(model)\n",
      "File \u001b[0;32m~/marbles/Projects/miniconda/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/loader.py:258\u001b[0m, in \u001b[0;36mFastLanguageModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m     tokenizer_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_get_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdispatch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_peft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m     model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(resize_model_vocab)\n",
      "File \u001b[0;32m~/marbles/Projects/miniconda/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/llama.py:1687\u001b[0m, in \u001b[0;36mFastLlamaModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[0m\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;66;03m# Cannot be None, since HF now checks for the config\u001b[39;00m\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit: kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m bnb_config\n\u001b[0;32m-> 1687\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# quantization_config     = bnb_config,\u001b[39;49;00m\n\u001b[1;32m   1692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meager\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1696\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1697\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;66;03m# Return old flag\u001b[39;00m\n\u001b[1;32m   1699\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHF_HUB_ENABLE_HF_TRANSFER\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m old_hf_transfer\n",
      "File \u001b[0;32m~/marbles/Projects/miniconda/envs/unsloth_env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/marbles/Projects/miniconda/envs/unsloth_env/lib/python3.11/site-packages/transformers/modeling_utils.py:4167\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4164\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[1;32m   4166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4167\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4170\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/marbles/Projects/miniconda/envs/unsloth_env/lib/python3.11/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:103\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    from unsloth import FastLanguageModel\n",
    "    from transformers import AutoTokenizer\n",
    "    \n",
    "    # Load the base model first\n",
    "    model = None\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"unsloth/Llama-3.3-70B-Instruct\",\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "        cache_dir=cache_dir,\n",
    "    )\n",
    "    \n",
    "    # Load the LoRA adapter weights\n",
    "    model = FastLanguageModel.get_peft_model(model)\n",
    "    model.load_adapter(\"lora_model\")\n",
    "    \n",
    "    # Load tokenizer separately\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")\n",
    "    \n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# alpaca_prompt = You MUST copy from above!\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    dataset[0][\"text\"].split(\"### Response:\\n\")[0]\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import json\n",
    "\n",
    "from typing import Dict, List, Any, Optional\n",
    "def extract_tp_from_text(text: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract the turning points from the model's generated text if it follows\n",
    "    the $\\boxed{...}$ format containing a JSON structure.\n",
    "    \"\"\"\n",
    "    pattern = r'\\$\\\\boxed\\{(.*?)\\}\\$'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        json_str = match.group(1)\n",
    "        # Convert the string to a standard JSON format\n",
    "        json_str = '{' + json_str.replace('\"', '\"') + '}'\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "### Final Turning Point Locations\n",
    "\n",
    "extract_tp_from_text(\"\"\"Based on the analysis, the final turning point locations in JSON format are:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"\"tp1\"\": 7,\n",
    "  \"\"tp2\"\": 10,\n",
    "  \"\"tp3\"\": 14,\n",
    "  \"\"tp4\"\": 19,\n",
    "  \"\"tp5\"\": 35\n",
    "}\n",
    "```\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
