{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE  # If needed\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Data Loading and Preparation\n",
    "with open(\"../datasets/ground_truth.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract turning points and arc labels\n",
    "x_ds = []\n",
    "y_ds = []\n",
    "\n",
    "for narrative in tqdm(data, desc=\"Processing narratives\"):\n",
    "    # Get turning points as features\n",
    "    tps = [\n",
    "        narrative['turning_points'].get(\"tp1\", 0),\n",
    "        narrative['turning_points'].get(\"tp2\", 0),\n",
    "        narrative['turning_points'].get(\"tp3\", 0),\n",
    "        narrative['turning_points'].get(\"tp4\", 0),\n",
    "        narrative['turning_points'].get(\"tp5\", 0)\n",
    "    ]\n",
    "    x_ds.append(tps)\n",
    "    \n",
    "    # Get arc label\n",
    "    y_ds.append(narrative.get(\"source\", \"Unknown\"))  # Handle unknown sources if any\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(x_ds, columns=['tp1', 'tp2', 'tp3', 'tp4', 'tp5'])\n",
    "df['source'] = y_ds\n",
    "\n",
    "# Handle 'Unknown' sources\n",
    "df = df[df['source'] != 'Unknown']\n",
    "\n",
    "# Feature Engineering\n",
    "# Differences between turning points\n",
    "df['diff_tp1_tp2'] = df['tp2'] - df['tp1']\n",
    "df['diff_tp1_tp3'] = df['tp3'] - df['tp1']\n",
    "df['diff_tp1_tp4'] = df['tp4'] - df['tp1']\n",
    "df['diff_tp1_tp5'] = df['tp5'] - df['tp1']\n",
    "df['diff_tp2_tp3'] = df['tp3'] - df['tp2']\n",
    "df['diff_tp2_tp4'] = df['tp4'] - df['tp2']\n",
    "df['diff_tp2_tp5'] = df['tp5'] - df['tp2']\n",
    "df['diff_tp3_tp4'] = df['tp4'] - df['tp3']\n",
    "df['diff_tp3_tp5'] = df['tp5'] - df['tp3']\n",
    "df['diff_tp4_tp5'] = df['tp5'] - df['tp4']\n",
    "\n",
    "# Example: Ratios\n",
    "df['ratio_tp1_tp5'] = df['tp1'] / df['tp5']\n",
    "df['ratio_tp2_tp4'] = df['tp2'] / df['tp4']\n",
    "\n",
    "# Select Features\n",
    "features = [\n",
    "    'tp1', 'tp2', 'tp3', 'tp4', 'tp5',\n",
    "    'diff_tp1_tp2', 'diff_tp1_tp3', 'diff_tp1_tp4', 'diff_tp1_tp5',\n",
    "    'diff_tp2_tp3', 'diff_tp2_tp4', 'diff_tp2_tp5',\n",
    "    'diff_tp3_tp4', 'diff_tp3_tp5',\n",
    "    'diff_tp4_tp5',\n",
    "    'ratio_tp1_tp5', 'ratio_tp2_tp4'\n",
    "]\n",
    "X = df[features].values\n",
    "y = df['source'].values\n",
    "\n",
    "# Label Encoding\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Optional: Handle Class Imbalance\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_scaled, y_encoded = smote.fit_resample(X_scaled, y_encoded)\n",
    "\n",
    "# 2. Model Definition and Hyperparameter Tuning\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],          # Reduced from 3 to 2 options\n",
    "    'learning_rate': [0.01, 0.1],        # Reduced from 3 to 2 options\n",
    "    'num_leaves': [31, 50],              # Reduced from 3 to 2 options\n",
    "    'max_depth': [-1, 10],               # Reduced from 3 to 2 options\n",
    "    'min_child_samples': [20],           # Reduced from 3 to 1 option\n",
    "    'subsample': [0.8],                  # Reduced from 3 to 1 option\n",
    "    'colsample_bytree': [0.8],           # Reduced from 3 to 1 option\n",
    "    'reg_alpha': [0.1],                  # Reduced from 3 to 1 option\n",
    "    'reg_lambda': [0.1]                  # Reduced from 3 to 1 option\n",
    "}\n",
    "# Initialize the LightGBM classifier\n",
    "lgbm_clf = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "# Initialize Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lgbm_clf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit Grid Search with progress bar\n",
    "with tqdm(total=1, desc=\"Grid Search\") as pbar:\n",
    "    grid_search.fit(X_scaled, y_encoded)\n",
    "    pbar.update(1)\n",
    "\n",
    "# Best Parameters and Score\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# 3. Evaluate the Best Model with Cross-Validation\n",
    "best_lgbm = grid_search.best_estimator_\n",
    "\n",
    "# Define Stratified K-Fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform Cross-Validation with progress bar\n",
    "cv_scores = []\n",
    "for train_idx, val_idx in tqdm(skf.split(X_scaled, y_encoded), total=5, desc=\"Cross-validation\"):\n",
    "    X_train_fold, X_val_fold = X_scaled[train_idx], X_scaled[val_idx]\n",
    "    y_train_fold, y_val_fold = y_encoded[train_idx], y_encoded[val_idx]\n",
    "    best_lgbm.fit(X_train_fold, y_train_fold)\n",
    "    score = best_lgbm.score(X_val_fold, y_val_fold)\n",
    "    cv_scores.append(score)\n",
    "\n",
    "cv_scores = np.array(cv_scores)\n",
    "print(f\"Final LightGBM Cross-Validation Accuracy: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
    "\n",
    "# 4. Final Model Training and Evaluation\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Train the best model on the entire training set with progress bar\n",
    "with tqdm(total=1, desc=\"Final Training\") as pbar:\n",
    "    best_lgbm.fit(X_train, y_train)\n",
    "    pbar.update(1)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_lgbm.predict(X_test)\n",
    "y_prob = best_lgbm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate Performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance\n",
    "importances = best_lgbm.feature_importances_\n",
    "feature_names = features\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance_df, palette='viridis')\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
